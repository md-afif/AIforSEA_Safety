{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should (intuitively) be some correlation between the 'over' columns and the 'outlier' columns \\\n",
    "After some rounds of validation, the training 'over' + 'outlier' together columns prove to be trash \n",
    "\n",
    "From running tree models, we can see that the most important features are the aggregated features \\\n",
    "Using this, we will try different combinations of features:\n",
    "1. Aggregated features alone (cols)\n",
    "2. Aggregated features + 'over' (cols2)\n",
    "3. Aggregated features + 'outlier' (cols3)\n",
    "\n",
    "The third combination gave the best results, hence hyperparameter tuning will be done on these features using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acceleration_mean</th>\n",
       "      <th>acceleration_median</th>\n",
       "      <th>acceleration_std</th>\n",
       "      <th>acceleration_spread</th>\n",
       "      <th>gyro_pc_mean</th>\n",
       "      <th>gyro_pc_median</th>\n",
       "      <th>gyro_pc_std</th>\n",
       "      <th>gyro_pc_spread</th>\n",
       "      <th>speed_mean</th>\n",
       "      <th>speed_median</th>\n",
       "      <th>...</th>\n",
       "      <th>second_median</th>\n",
       "      <th>second_std</th>\n",
       "      <th>second_spread</th>\n",
       "      <th>num_non_speed_outlier</th>\n",
       "      <th>num_speed_outlier</th>\n",
       "      <th>num_non_accel_outlier</th>\n",
       "      <th>num_accel_outlier</th>\n",
       "      <th>num_non_gyro_outlier</th>\n",
       "      <th>num_gyro_outlier</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.883337</td>\n",
       "      <td>9.852269</td>\n",
       "      <td>0.619492</td>\n",
       "      <td>6.530989</td>\n",
       "      <td>-0.006583</td>\n",
       "      <td>-0.002863</td>\n",
       "      <td>0.099002</td>\n",
       "      <td>1.101352</td>\n",
       "      <td>9.003204</td>\n",
       "      <td>8.503366</td>\n",
       "      <td>...</td>\n",
       "      <td>1086.5</td>\n",
       "      <td>534.113894</td>\n",
       "      <td>1589.0</td>\n",
       "      <td>243</td>\n",
       "      <td>6</td>\n",
       "      <td>235</td>\n",
       "      <td>14</td>\n",
       "      <td>237</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.865608</td>\n",
       "      <td>9.847932</td>\n",
       "      <td>0.522142</td>\n",
       "      <td>5.819621</td>\n",
       "      <td>-0.006855</td>\n",
       "      <td>-0.003612</td>\n",
       "      <td>0.090770</td>\n",
       "      <td>1.123587</td>\n",
       "      <td>8.019369</td>\n",
       "      <td>7.206634</td>\n",
       "      <td>...</td>\n",
       "      <td>607.5</td>\n",
       "      <td>289.129088</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>192</td>\n",
       "      <td>16</td>\n",
       "      <td>206</td>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9.929590</td>\n",
       "      <td>9.877755</td>\n",
       "      <td>0.515173</td>\n",
       "      <td>5.168422</td>\n",
       "      <td>-0.012751</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.117109</td>\n",
       "      <td>0.896289</td>\n",
       "      <td>3.157213</td>\n",
       "      <td>2.998761</td>\n",
       "      <td>...</td>\n",
       "      <td>97.0</td>\n",
       "      <td>356.319445</td>\n",
       "      <td>825.0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9.813434</td>\n",
       "      <td>9.791035</td>\n",
       "      <td>0.620066</td>\n",
       "      <td>13.349284</td>\n",
       "      <td>0.022429</td>\n",
       "      <td>0.024239</td>\n",
       "      <td>0.112628</td>\n",
       "      <td>1.166471</td>\n",
       "      <td>6.150996</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>...</td>\n",
       "      <td>547.5</td>\n",
       "      <td>315.962793</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>259</td>\n",
       "      <td>13</td>\n",
       "      <td>262</td>\n",
       "      <td>10</td>\n",
       "      <td>247</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9.918090</td>\n",
       "      <td>9.904142</td>\n",
       "      <td>0.585346</td>\n",
       "      <td>7.280114</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.004189</td>\n",
       "      <td>0.106469</td>\n",
       "      <td>1.161481</td>\n",
       "      <td>4.628921</td>\n",
       "      <td>1.936962</td>\n",
       "      <td>...</td>\n",
       "      <td>547.0</td>\n",
       "      <td>316.243577</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>259</td>\n",
       "      <td>13</td>\n",
       "      <td>262</td>\n",
       "      <td>10</td>\n",
       "      <td>248</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>9.826470</td>\n",
       "      <td>9.789800</td>\n",
       "      <td>0.916836</td>\n",
       "      <td>8.572037</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>-0.002687</td>\n",
       "      <td>0.072664</td>\n",
       "      <td>0.903745</td>\n",
       "      <td>12.176386</td>\n",
       "      <td>13.017325</td>\n",
       "      <td>...</td>\n",
       "      <td>481.0</td>\n",
       "      <td>276.761488</td>\n",
       "      <td>959.0</td>\n",
       "      <td>205</td>\n",
       "      <td>30</td>\n",
       "      <td>192</td>\n",
       "      <td>43</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>9.763213</td>\n",
       "      <td>9.646309</td>\n",
       "      <td>0.730155</td>\n",
       "      <td>9.416841</td>\n",
       "      <td>-0.000840</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.078446</td>\n",
       "      <td>0.754180</td>\n",
       "      <td>5.384260</td>\n",
       "      <td>3.540000</td>\n",
       "      <td>...</td>\n",
       "      <td>268.0</td>\n",
       "      <td>130.510496</td>\n",
       "      <td>462.0</td>\n",
       "      <td>92</td>\n",
       "      <td>3</td>\n",
       "      <td>89</td>\n",
       "      <td>6</td>\n",
       "      <td>87</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>9.550677</td>\n",
       "      <td>9.494390</td>\n",
       "      <td>0.833292</td>\n",
       "      <td>9.474737</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>-0.000612</td>\n",
       "      <td>0.110181</td>\n",
       "      <td>0.909695</td>\n",
       "      <td>8.702027</td>\n",
       "      <td>9.580000</td>\n",
       "      <td>...</td>\n",
       "      <td>187.0</td>\n",
       "      <td>108.397417</td>\n",
       "      <td>374.0</td>\n",
       "      <td>87</td>\n",
       "      <td>5</td>\n",
       "      <td>77</td>\n",
       "      <td>15</td>\n",
       "      <td>83</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9.948639</td>\n",
       "      <td>9.877962</td>\n",
       "      <td>0.750480</td>\n",
       "      <td>5.686104</td>\n",
       "      <td>-0.004018</td>\n",
       "      <td>-0.003111</td>\n",
       "      <td>0.151980</td>\n",
       "      <td>0.988519</td>\n",
       "      <td>6.659024</td>\n",
       "      <td>5.192059</td>\n",
       "      <td>...</td>\n",
       "      <td>112.0</td>\n",
       "      <td>93.043769</td>\n",
       "      <td>299.0</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9.873517</td>\n",
       "      <td>9.823053</td>\n",
       "      <td>0.425662</td>\n",
       "      <td>5.916028</td>\n",
       "      <td>-0.002192</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.082987</td>\n",
       "      <td>0.767631</td>\n",
       "      <td>4.152211</td>\n",
       "      <td>3.702154</td>\n",
       "      <td>...</td>\n",
       "      <td>238.0</td>\n",
       "      <td>177.047431</td>\n",
       "      <td>555.0</td>\n",
       "      <td>100</td>\n",
       "      <td>9</td>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "      <td>104</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   acceleration_mean  acceleration_median  acceleration_std  \\\n",
       "0           9.883337             9.852269          0.619492   \n",
       "1           9.865608             9.847932          0.522142   \n",
       "2           9.929590             9.877755          0.515173   \n",
       "3           9.813434             9.791035          0.620066   \n",
       "4           9.918090             9.904142          0.585346   \n",
       "5           9.826470             9.789800          0.916836   \n",
       "6           9.763213             9.646309          0.730155   \n",
       "7           9.550677             9.494390          0.833292   \n",
       "8           9.948639             9.877962          0.750480   \n",
       "9           9.873517             9.823053          0.425662   \n",
       "\n",
       "   acceleration_spread  gyro_pc_mean  gyro_pc_median  gyro_pc_std  \\\n",
       "0             6.530989     -0.006583       -0.002863     0.099002   \n",
       "1             5.819621     -0.006855       -0.003612     0.090770   \n",
       "2             5.168422     -0.012751        0.001369     0.117109   \n",
       "3            13.349284      0.022429        0.024239     0.112628   \n",
       "4             7.280114      0.000480        0.004189     0.106469   \n",
       "5             8.572037      0.002651       -0.002687     0.072664   \n",
       "6             9.416841     -0.000840        0.000250     0.078446   \n",
       "7             9.474737      0.001922       -0.000612     0.110181   \n",
       "8             5.686104     -0.004018       -0.003111     0.151980   \n",
       "9             5.916028     -0.002192        0.000388     0.082987   \n",
       "\n",
       "   gyro_pc_spread  speed_mean  speed_median  ...  second_median  second_std  \\\n",
       "0        1.101352    9.003204      8.503366  ...         1086.5  534.113894   \n",
       "1        1.123587    8.019369      7.206634  ...          607.5  289.129088   \n",
       "2        0.896289    3.157213      2.998761  ...           97.0  356.319445   \n",
       "3        1.166471    6.150996      3.310000  ...          547.5  315.962793   \n",
       "4        1.161481    4.628921      1.936962  ...          547.0  316.243577   \n",
       "5        0.903745   12.176386     13.017325  ...          481.0  276.761488   \n",
       "6        0.754180    5.384260      3.540000  ...          268.0  130.510496   \n",
       "7        0.909695    8.702027      9.580000  ...          187.0  108.397417   \n",
       "8        0.988519    6.659024      5.192059  ...          112.0   93.043769   \n",
       "9        0.767631    4.152211      3.702154  ...          238.0  177.047431   \n",
       "\n",
       "   second_spread  num_non_speed_outlier  num_speed_outlier  \\\n",
       "0         1589.0                    243                  6   \n",
       "1         1034.0                    192                 16   \n",
       "2          825.0                     47                  0   \n",
       "3         1094.0                    259                 13   \n",
       "4         1094.0                    259                 13   \n",
       "5          959.0                    205                 30   \n",
       "6          462.0                     92                  3   \n",
       "7          374.0                     87                  5   \n",
       "8          299.0                     48                  6   \n",
       "9          555.0                    100                  9   \n",
       "\n",
       "   num_non_accel_outlier  num_accel_outlier  num_non_gyro_outlier  \\\n",
       "0                    235                 14                   237   \n",
       "1                    206                  2                   197   \n",
       "2                     47                  0                    41   \n",
       "3                    262                 10                   247   \n",
       "4                    262                 10                   248   \n",
       "5                    192                 43                   235   \n",
       "6                     89                  6                    87   \n",
       "7                     77                 15                    83   \n",
       "8                     49                  5                    45   \n",
       "9                    106                  3                   104   \n",
       "\n",
       "   num_gyro_outlier  label  \n",
       "0                12      0  \n",
       "1                11      1  \n",
       "2                 6      1  \n",
       "3                25      1  \n",
       "4                24      0  \n",
       "5                 0      0  \n",
       "6                 8      0  \n",
       "7                 9      0  \n",
       "8                 9      0  \n",
       "9                 5      0  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), '../data/safety/total_df.csv')\n",
    "\n",
    "# Without 'over' and 'outlier'\n",
    "cols = ['acceleration_mean', 'acceleration_median', 'acceleration_std',\n",
    "       'acceleration_spread', 'gyro_pc_mean', 'gyro_pc_median', 'gyro_pc_std',\n",
    "       'gyro_pc_spread', 'speed_mean', 'speed_median', 'speed_std',\n",
    "       'speed_spread', 'second_mean', 'second_median', 'second_std',\n",
    "       'second_spread', 'label']\n",
    "\n",
    "# Without 'outlier'\n",
    "cols2 = ['acceleration_mean', 'acceleration_median', 'acceleration_std',\n",
    "       'acceleration_spread', 'gyro_pc_mean', 'gyro_pc_median', 'gyro_pc_std',\n",
    "       'gyro_pc_spread', 'speed_mean', 'speed_median', 'speed_std',\n",
    "       'speed_spread', 'second_mean', 'second_median', 'second_std',\n",
    "       'second_spread', 'over_speed', 'over_second', 'over_acceleration_x',\n",
    "       'over_acceleration_y', 'over_acceleration_z', 'over_gyro_x',\n",
    "       'over_gyro_y', 'over_gyro_z', 'label']\n",
    "\n",
    "# Without 'over'\n",
    "cols3 = ['acceleration_mean', 'acceleration_median', 'acceleration_std',\n",
    "       'acceleration_spread', 'gyro_pc_mean', 'gyro_pc_median', 'gyro_pc_std',\n",
    "       'gyro_pc_spread', 'speed_mean', 'speed_median', 'speed_std',\n",
    "       'speed_spread', 'second_mean', 'second_median', 'second_std',\n",
    "       'second_spread', 'num_non_speed_outlier',\n",
    "       'num_speed_outlier', 'num_non_accel_outlier', 'num_accel_outlier',\n",
    "       'num_non_gyro_outlier', 'num_gyro_outlier', 'label']\n",
    "\n",
    "\n",
    "agg_diff_df = pd.read_csv(DATA_DIR)\n",
    "agg_diff_df = agg_diff_df.drop('bookingid', axis='columns')\n",
    "agg_diff_df = agg_diff_df[cols3]\n",
    "agg_diff_df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling\n",
    "\n",
    "Downsample to ensure that the neural network learns to classify equally, and not focusing on one class etc \\\n",
    "Train set will have 50:50 distribution, test set will follow actual distribution of 75:25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist = agg_diff_df['label'].value_counts()\n",
    "# actual_dist = dist[1] / (dist[0] + dist[1]) \n",
    "# actual_dist # About 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling training set to get equal class distribution\n",
    "seed = 199\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "count_0 = agg_diff_df['label'].value_counts()[0]\n",
    "count_1 = agg_diff_df['label'].value_counts()[1]\n",
    "\n",
    "idx0 = agg_diff_df[agg_diff_df['label'] == 0].index.values\n",
    "sample_0_idx = np.random.choice(idx0, count_1)\n",
    "\n",
    "df_0 = agg_diff_df.iloc[sample_0_idx, :]\n",
    "downsample_df = pd.concat([df_0, agg_diff_df[agg_diff_df['label'] == 1]]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = downsample_df.drop('label', axis=1)\n",
    "y = downsample_df['label']\n",
    "\n",
    "X_train, X_test2, y_train, y_test2 = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reforming test set to have class distribution of 75% class 0 : 25% class 1\n",
    "prop = count_1 / (count_0 + count_1)\n",
    "sample_0_idx2 = [idx for idx in idx0 if idx not in sample_0_idx]\n",
    "\n",
    "y_count = y_test2.value_counts()[1]\n",
    "sample_0_idx2 = np.random.choice(sample_0_idx2, size=np.int((y_count / 25) * 75))\n",
    "\n",
    "new_df0 = agg_diff_df.iloc[sample_0_idx2, :].sample(n=np.int((y_count / 25) * 75))\n",
    "test = pd.merge(X_test2, y_test2, left_index=True, right_index=True)\n",
    "test = test[test['label'] == 1]\n",
    "\n",
    "df_merge = pd.concat([new_df0, test], axis=0).reset_index(drop=True).sample(frac=1)\n",
    "\n",
    "X_test, y_test = df_merge.drop('label', axis=1), df_merge['label']\n",
    "\n",
    "del X_test2, y_test2, df_merge, test, new_df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training class distribution : \n",
      "1    3999\n",
      "0    3940\n",
      "Name: label, dtype: int64\n",
      "Testing class distribution : \n",
      "0    2889\n",
      "1     963\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking distributions\n",
    "print(\"Training class distribution : \")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"Testing class distribution : \")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-downsampling approach\n",
    "\n",
    "Leaving distribution as is then see how it goes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 13\n",
    "\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# X = agg_diff_df.drop('label', axis=1)\n",
    "# y = agg_diff_df['label']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "#                                                     random_state=seed, shuffle=True)\n",
    "\n",
    "# print(y_train.value_counts())\n",
    "# print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardising data to have mean = 0 and standard deviation = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acceleration_mean</th>\n",
       "      <th>acceleration_median</th>\n",
       "      <th>acceleration_std</th>\n",
       "      <th>acceleration_spread</th>\n",
       "      <th>gyro_pc_mean</th>\n",
       "      <th>gyro_pc_median</th>\n",
       "      <th>gyro_pc_std</th>\n",
       "      <th>gyro_pc_spread</th>\n",
       "      <th>speed_mean</th>\n",
       "      <th>speed_median</th>\n",
       "      <th>...</th>\n",
       "      <th>second_mean</th>\n",
       "      <th>second_median</th>\n",
       "      <th>second_std</th>\n",
       "      <th>second_spread</th>\n",
       "      <th>num_non_speed_outlier</th>\n",
       "      <th>num_speed_outlier</th>\n",
       "      <th>num_non_accel_outlier</th>\n",
       "      <th>num_accel_outlier</th>\n",
       "      <th>num_non_gyro_outlier</th>\n",
       "      <th>num_gyro_outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>7.939000e+03</td>\n",
       "      <td>7.939000e+03</td>\n",
       "      <td>7.939000e+03</td>\n",
       "      <td>7.939000e+03</td>\n",
       "      <td>7.939000e+03</td>\n",
       "      <td>7.939000e+03</td>\n",
       "      <td>7.939000e+03</td>\n",
       "      <td>7.939000e+03</td>\n",
       "      <td>7.939000e+03</td>\n",
       "      <td>7.939000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>7.939000e+03</td>\n",
       "      <td>7.939000e+03</td>\n",
       "      <td>7.939000e+03</td>\n",
       "      <td>7.939000e+03</td>\n",
       "      <td>7.939000e+03</td>\n",
       "      <td>7.939000e+03</td>\n",
       "      <td>7.939000e+03</td>\n",
       "      <td>7.939000e+03</td>\n",
       "      <td>7.939000e+03</td>\n",
       "      <td>7.939000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>-1.117997e-15</td>\n",
       "      <td>-3.582131e-16</td>\n",
       "      <td>-5.544822e-17</td>\n",
       "      <td>-1.301040e-16</td>\n",
       "      <td>2.805362e-17</td>\n",
       "      <td>-5.848874e-18</td>\n",
       "      <td>-5.993023e-17</td>\n",
       "      <td>-6.174121e-18</td>\n",
       "      <td>3.027906e-16</td>\n",
       "      <td>6.368504e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>1.164902e-17</td>\n",
       "      <td>-1.552271e-17</td>\n",
       "      <td>6.267817e-17</td>\n",
       "      <td>3.945005e-17</td>\n",
       "      <td>9.817062e-18</td>\n",
       "      <td>-1.298034e-16</td>\n",
       "      <td>-3.865293e-17</td>\n",
       "      <td>9.133224e-17</td>\n",
       "      <td>7.141843e-17</td>\n",
       "      <td>-2.261455e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>1.000063e+00</td>\n",
       "      <td>1.000063e+00</td>\n",
       "      <td>1.000063e+00</td>\n",
       "      <td>1.000063e+00</td>\n",
       "      <td>1.000063e+00</td>\n",
       "      <td>1.000063e+00</td>\n",
       "      <td>1.000063e+00</td>\n",
       "      <td>1.000063e+00</td>\n",
       "      <td>1.000063e+00</td>\n",
       "      <td>1.000063e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000063e+00</td>\n",
       "      <td>1.000063e+00</td>\n",
       "      <td>1.000063e+00</td>\n",
       "      <td>1.000063e+00</td>\n",
       "      <td>1.000063e+00</td>\n",
       "      <td>1.000063e+00</td>\n",
       "      <td>1.000063e+00</td>\n",
       "      <td>1.000063e+00</td>\n",
       "      <td>1.000063e+00</td>\n",
       "      <td>1.000063e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>-1.403143e+01</td>\n",
       "      <td>-1.487927e+01</td>\n",
       "      <td>-1.263256e+00</td>\n",
       "      <td>-1.265030e+00</td>\n",
       "      <td>-7.505363e+01</td>\n",
       "      <td>-7.457329e+01</td>\n",
       "      <td>-6.390963e-01</td>\n",
       "      <td>-4.769338e-01</td>\n",
       "      <td>-1.987609e+00</td>\n",
       "      <td>-1.409070e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.755744e+00</td>\n",
       "      <td>-1.718727e+00</td>\n",
       "      <td>-1.882765e+00</td>\n",
       "      <td>-1.878490e+00</td>\n",
       "      <td>-1.775224e+00</td>\n",
       "      <td>-8.030272e-01</td>\n",
       "      <td>-1.709059e+00</td>\n",
       "      <td>-5.800865e-01</td>\n",
       "      <td>-1.716260e+00</td>\n",
       "      <td>-7.754690e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>-1.389243e-01</td>\n",
       "      <td>-1.059107e-01</td>\n",
       "      <td>-4.052873e-01</td>\n",
       "      <td>-4.342546e-01</td>\n",
       "      <td>-2.422009e-02</td>\n",
       "      <td>-8.605665e-03</td>\n",
       "      <td>-2.943770e-01</td>\n",
       "      <td>-2.756335e-01</td>\n",
       "      <td>-7.383328e-01</td>\n",
       "      <td>-7.663352e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.264760e-01</td>\n",
       "      <td>-7.311748e-01</td>\n",
       "      <td>-7.227561e-01</td>\n",
       "      <td>-7.502336e-01</td>\n",
       "      <td>-7.481078e-01</td>\n",
       "      <td>-5.755529e-01</td>\n",
       "      <td>-7.528046e-01</td>\n",
       "      <td>-5.018448e-01</td>\n",
       "      <td>-7.760707e-01</td>\n",
       "      <td>-4.570796e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>-1.701149e-02</td>\n",
       "      <td>1.429840e-02</td>\n",
       "      <td>-1.874991e-01</td>\n",
       "      <td>-1.923706e-01</td>\n",
       "      <td>9.958785e-04</td>\n",
       "      <td>-1.561686e-03</td>\n",
       "      <td>-1.994687e-01</td>\n",
       "      <td>-2.314391e-01</td>\n",
       "      <td>-2.150629e-01</td>\n",
       "      <td>-2.388728e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.396704e-01</td>\n",
       "      <td>-1.343923e-01</td>\n",
       "      <td>-1.233851e-01</td>\n",
       "      <td>-1.365864e-01</td>\n",
       "      <td>-1.625555e-01</td>\n",
       "      <td>-3.101662e-01</td>\n",
       "      <td>-1.459509e-01</td>\n",
       "      <td>-3.453614e-01</td>\n",
       "      <td>-1.584955e-01</td>\n",
       "      <td>-2.448200e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>1.127595e-01</td>\n",
       "      <td>1.334502e-01</td>\n",
       "      <td>9.040556e-02</td>\n",
       "      <td>1.231454e-01</td>\n",
       "      <td>2.565321e-02</td>\n",
       "      <td>7.447437e-03</td>\n",
       "      <td>-7.748910e-02</td>\n",
       "      <td>-1.606479e-01</td>\n",
       "      <td>6.101081e-01</td>\n",
       "      <td>6.079683e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>5.685832e-01</td>\n",
       "      <td>5.514987e-01</td>\n",
       "      <td>6.032076e-01</td>\n",
       "      <td>6.188211e-01</td>\n",
       "      <td>6.149811e-01</td>\n",
       "      <td>1.447824e-01</td>\n",
       "      <td>6.080188e-01</td>\n",
       "      <td>4.584714e-02</td>\n",
       "      <td>6.342130e-01</td>\n",
       "      <td>7.356943e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>4.721768e+01</td>\n",
       "      <td>4.991402e+01</td>\n",
       "      <td>1.586540e+01</td>\n",
       "      <td>1.694057e+01</td>\n",
       "      <td>3.429427e+01</td>\n",
       "      <td>3.467825e+01</td>\n",
       "      <td>2.857527e+01</td>\n",
       "      <td>1.822551e+01</td>\n",
       "      <td>4.312907e+00</td>\n",
       "      <td>3.527370e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.518335e+01</td>\n",
       "      <td>1.361776e+01</td>\n",
       "      <td>1.412638e+01</td>\n",
       "      <td>1.407828e+01</td>\n",
       "      <td>1.571455e+01</td>\n",
       "      <td>8.030558e+00</td>\n",
       "      <td>1.391283e+01</td>\n",
       "      <td>1.269492e+01</td>\n",
       "      <td>1.147404e+01</td>\n",
       "      <td>1.507325e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       acceleration_mean  acceleration_median  acceleration_std  \\\n",
       "count       7.939000e+03         7.939000e+03      7.939000e+03   \n",
       "mean       -1.117997e-15        -3.582131e-16     -5.544822e-17   \n",
       "std         1.000063e+00         1.000063e+00      1.000063e+00   \n",
       "min        -1.403143e+01        -1.487927e+01     -1.263256e+00   \n",
       "25%        -1.389243e-01        -1.059107e-01     -4.052873e-01   \n",
       "50%        -1.701149e-02         1.429840e-02     -1.874991e-01   \n",
       "75%         1.127595e-01         1.334502e-01      9.040556e-02   \n",
       "max         4.721768e+01         4.991402e+01      1.586540e+01   \n",
       "\n",
       "       acceleration_spread  gyro_pc_mean  gyro_pc_median   gyro_pc_std  \\\n",
       "count         7.939000e+03  7.939000e+03    7.939000e+03  7.939000e+03   \n",
       "mean         -1.301040e-16  2.805362e-17   -5.848874e-18 -5.993023e-17   \n",
       "std           1.000063e+00  1.000063e+00    1.000063e+00  1.000063e+00   \n",
       "min          -1.265030e+00 -7.505363e+01   -7.457329e+01 -6.390963e-01   \n",
       "25%          -4.342546e-01 -2.422009e-02   -8.605665e-03 -2.943770e-01   \n",
       "50%          -1.923706e-01  9.958785e-04   -1.561686e-03 -1.994687e-01   \n",
       "75%           1.231454e-01  2.565321e-02    7.447437e-03 -7.748910e-02   \n",
       "max           1.694057e+01  3.429427e+01    3.467825e+01  2.857527e+01   \n",
       "\n",
       "       gyro_pc_spread    speed_mean  speed_median  ...   second_mean  \\\n",
       "count    7.939000e+03  7.939000e+03  7.939000e+03  ...  7.939000e+03   \n",
       "mean    -6.174121e-18  3.027906e-16  6.368504e-17  ...  1.164902e-17   \n",
       "std      1.000063e+00  1.000063e+00  1.000063e+00  ...  1.000063e+00   \n",
       "min     -4.769338e-01 -1.987609e+00 -1.409070e+00  ... -1.755744e+00   \n",
       "25%     -2.756335e-01 -7.383328e-01 -7.663352e-01  ... -7.264760e-01   \n",
       "50%     -2.314391e-01 -2.150629e-01 -2.388728e-01  ... -1.396704e-01   \n",
       "75%     -1.606479e-01  6.101081e-01  6.079683e-01  ...  5.685832e-01   \n",
       "max      1.822551e+01  4.312907e+00  3.527370e+00  ...  1.518335e+01   \n",
       "\n",
       "       second_median    second_std  second_spread  num_non_speed_outlier  \\\n",
       "count   7.939000e+03  7.939000e+03   7.939000e+03           7.939000e+03   \n",
       "mean   -1.552271e-17  6.267817e-17   3.945005e-17           9.817062e-18   \n",
       "std     1.000063e+00  1.000063e+00   1.000063e+00           1.000063e+00   \n",
       "min    -1.718727e+00 -1.882765e+00  -1.878490e+00          -1.775224e+00   \n",
       "25%    -7.311748e-01 -7.227561e-01  -7.502336e-01          -7.481078e-01   \n",
       "50%    -1.343923e-01 -1.233851e-01  -1.365864e-01          -1.625555e-01   \n",
       "75%     5.514987e-01  6.032076e-01   6.188211e-01           6.149811e-01   \n",
       "max     1.361776e+01  1.412638e+01   1.407828e+01           1.571455e+01   \n",
       "\n",
       "       num_speed_outlier  num_non_accel_outlier  num_accel_outlier  \\\n",
       "count       7.939000e+03           7.939000e+03       7.939000e+03   \n",
       "mean       -1.298034e-16          -3.865293e-17       9.133224e-17   \n",
       "std         1.000063e+00           1.000063e+00       1.000063e+00   \n",
       "min        -8.030272e-01          -1.709059e+00      -5.800865e-01   \n",
       "25%        -5.755529e-01          -7.528046e-01      -5.018448e-01   \n",
       "50%        -3.101662e-01          -1.459509e-01      -3.453614e-01   \n",
       "75%         1.447824e-01           6.080188e-01       4.584714e-02   \n",
       "max         8.030558e+00           1.391283e+01       1.269492e+01   \n",
       "\n",
       "       num_non_gyro_outlier  num_gyro_outlier  \n",
       "count          7.939000e+03      7.939000e+03  \n",
       "mean           7.141843e-17     -2.261455e-16  \n",
       "std            1.000063e+00      1.000063e+00  \n",
       "min           -1.716260e+00     -7.754690e-01  \n",
       "25%           -7.760707e-01     -4.570796e-01  \n",
       "50%           -1.584955e-01     -2.448200e-01  \n",
       "75%            6.342130e-01      7.356943e-02  \n",
       "max            1.147404e+01      1.507325e+01  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_train_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acceleration_mean</th>\n",
       "      <th>acceleration_median</th>\n",
       "      <th>acceleration_std</th>\n",
       "      <th>acceleration_spread</th>\n",
       "      <th>gyro_pc_mean</th>\n",
       "      <th>gyro_pc_median</th>\n",
       "      <th>gyro_pc_std</th>\n",
       "      <th>gyro_pc_spread</th>\n",
       "      <th>speed_mean</th>\n",
       "      <th>speed_median</th>\n",
       "      <th>...</th>\n",
       "      <th>second_mean</th>\n",
       "      <th>second_median</th>\n",
       "      <th>second_std</th>\n",
       "      <th>second_spread</th>\n",
       "      <th>num_non_speed_outlier</th>\n",
       "      <th>num_speed_outlier</th>\n",
       "      <th>num_non_accel_outlier</th>\n",
       "      <th>num_accel_outlier</th>\n",
       "      <th>num_non_gyro_outlier</th>\n",
       "      <th>num_gyro_outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>3.852000e+03</td>\n",
       "      <td>3.852000e+03</td>\n",
       "      <td>3.852000e+03</td>\n",
       "      <td>3.852000e+03</td>\n",
       "      <td>3.852000e+03</td>\n",
       "      <td>3.852000e+03</td>\n",
       "      <td>3.852000e+03</td>\n",
       "      <td>3.852000e+03</td>\n",
       "      <td>3.852000e+03</td>\n",
       "      <td>3.852000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>3.852000e+03</td>\n",
       "      <td>3.852000e+03</td>\n",
       "      <td>3.852000e+03</td>\n",
       "      <td>3.852000e+03</td>\n",
       "      <td>3.852000e+03</td>\n",
       "      <td>3.852000e+03</td>\n",
       "      <td>3.852000e+03</td>\n",
       "      <td>3.852000e+03</td>\n",
       "      <td>3.852000e+03</td>\n",
       "      <td>3.852000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>-5.931926e-17</td>\n",
       "      <td>-9.702851e-16</td>\n",
       "      <td>2.048127e-16</td>\n",
       "      <td>-1.195032e-17</td>\n",
       "      <td>-1.862621e-17</td>\n",
       "      <td>1.458393e-17</td>\n",
       "      <td>-1.572663e-16</td>\n",
       "      <td>6.953305e-18</td>\n",
       "      <td>-2.386893e-16</td>\n",
       "      <td>-1.102081e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.325812e-17</td>\n",
       "      <td>-4.052372e-17</td>\n",
       "      <td>-2.556510e-17</td>\n",
       "      <td>-1.317165e-16</td>\n",
       "      <td>1.972001e-16</td>\n",
       "      <td>1.158644e-17</td>\n",
       "      <td>3.585455e-17</td>\n",
       "      <td>-4.340592e-17</td>\n",
       "      <td>1.339070e-16</td>\n",
       "      <td>-3.594823e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>1.000130e+00</td>\n",
       "      <td>1.000130e+00</td>\n",
       "      <td>1.000130e+00</td>\n",
       "      <td>1.000130e+00</td>\n",
       "      <td>1.000130e+00</td>\n",
       "      <td>1.000130e+00</td>\n",
       "      <td>1.000130e+00</td>\n",
       "      <td>1.000130e+00</td>\n",
       "      <td>1.000130e+00</td>\n",
       "      <td>1.000130e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000130e+00</td>\n",
       "      <td>1.000130e+00</td>\n",
       "      <td>1.000130e+00</td>\n",
       "      <td>1.000130e+00</td>\n",
       "      <td>1.000130e+00</td>\n",
       "      <td>1.000130e+00</td>\n",
       "      <td>1.000130e+00</td>\n",
       "      <td>1.000130e+00</td>\n",
       "      <td>1.000130e+00</td>\n",
       "      <td>1.000130e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>-9.517856e+00</td>\n",
       "      <td>-9.678875e+00</td>\n",
       "      <td>-1.406150e+00</td>\n",
       "      <td>-1.379592e+00</td>\n",
       "      <td>-1.043166e+01</td>\n",
       "      <td>-9.735176e+00</td>\n",
       "      <td>-7.270629e-01</td>\n",
       "      <td>-4.629994e-01</td>\n",
       "      <td>-1.865650e+00</td>\n",
       "      <td>-1.395112e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.602576e+00</td>\n",
       "      <td>-1.568730e+00</td>\n",
       "      <td>-1.854968e+00</td>\n",
       "      <td>-1.842653e+00</td>\n",
       "      <td>-1.761975e+00</td>\n",
       "      <td>-7.863839e-01</td>\n",
       "      <td>-1.724276e+00</td>\n",
       "      <td>-5.614917e-01</td>\n",
       "      <td>-1.710210e+00</td>\n",
       "      <td>-7.886986e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>-9.932039e-02</td>\n",
       "      <td>-7.957876e-02</td>\n",
       "      <td>-4.246305e-01</td>\n",
       "      <td>-4.625899e-01</td>\n",
       "      <td>-3.243065e-01</td>\n",
       "      <td>-1.786610e-01</td>\n",
       "      <td>-3.213616e-01</td>\n",
       "      <td>-2.587445e-01</td>\n",
       "      <td>-7.470991e-01</td>\n",
       "      <td>-7.792574e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.799369e-01</td>\n",
       "      <td>-6.968992e-01</td>\n",
       "      <td>-7.427452e-01</td>\n",
       "      <td>-7.444038e-01</td>\n",
       "      <td>-7.706154e-01</td>\n",
       "      <td>-5.618027e-01</td>\n",
       "      <td>-7.563422e-01</td>\n",
       "      <td>-4.780220e-01</td>\n",
       "      <td>-7.737081e-01</td>\n",
       "      <td>-4.612986e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>-1.508900e-02</td>\n",
       "      <td>-2.272165e-03</td>\n",
       "      <td>-1.587854e-01</td>\n",
       "      <td>-1.717348e-01</td>\n",
       "      <td>-1.138947e-02</td>\n",
       "      <td>-5.136919e-02</td>\n",
       "      <td>-1.958964e-01</td>\n",
       "      <td>-2.089942e-01</td>\n",
       "      <td>-2.424808e-01</td>\n",
       "      <td>-2.662417e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.474542e-01</td>\n",
       "      <td>-1.492960e-01</td>\n",
       "      <td>-1.370580e-01</td>\n",
       "      <td>-1.459317e-01</td>\n",
       "      <td>-1.716688e-01</td>\n",
       "      <td>-3.176928e-01</td>\n",
       "      <td>-1.637299e-01</td>\n",
       "      <td>-3.110827e-01</td>\n",
       "      <td>-1.723753e-01</td>\n",
       "      <td>-2.157487e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>6.830606e-02</td>\n",
       "      <td>7.102765e-02</td>\n",
       "      <td>1.681711e-01</td>\n",
       "      <td>1.762076e-01</td>\n",
       "      <td>2.882491e-01</td>\n",
       "      <td>1.208569e-01</td>\n",
       "      <td>-2.988886e-02</td>\n",
       "      <td>-1.315703e-01</td>\n",
       "      <td>5.971829e-01</td>\n",
       "      <td>6.360827e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>5.535852e-01</td>\n",
       "      <td>5.266646e-01</td>\n",
       "      <td>6.472530e-01</td>\n",
       "      <td>6.609556e-01</td>\n",
       "      <td>6.234845e-01</td>\n",
       "      <td>1.509984e-01</td>\n",
       "      <td>6.190121e-01</td>\n",
       "      <td>5.061919e-02</td>\n",
       "      <td>6.359736e-01</td>\n",
       "      <td>1.116513e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>3.184967e+01</td>\n",
       "      <td>3.233498e+01</td>\n",
       "      <td>1.769901e+01</td>\n",
       "      <td>1.528644e+01</td>\n",
       "      <td>1.382471e+01</td>\n",
       "      <td>1.179318e+01</td>\n",
       "      <td>1.604057e+01</td>\n",
       "      <td>2.192246e+01</td>\n",
       "      <td>3.946116e+00</td>\n",
       "      <td>3.199149e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.370491e+01</td>\n",
       "      <td>2.264750e+01</td>\n",
       "      <td>6.607630e+00</td>\n",
       "      <td>6.626252e+00</td>\n",
       "      <td>7.748884e+00</td>\n",
       "      <td>1.085278e+01</td>\n",
       "      <td>7.678506e+00</td>\n",
       "      <td>1.051215e+01</td>\n",
       "      <td>7.704098e+00</td>\n",
       "      <td>1.652257e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       acceleration_mean  acceleration_median  acceleration_std  \\\n",
       "count       3.852000e+03         3.852000e+03      3.852000e+03   \n",
       "mean       -5.931926e-17        -9.702851e-16      2.048127e-16   \n",
       "std         1.000130e+00         1.000130e+00      1.000130e+00   \n",
       "min        -9.517856e+00        -9.678875e+00     -1.406150e+00   \n",
       "25%        -9.932039e-02        -7.957876e-02     -4.246305e-01   \n",
       "50%        -1.508900e-02        -2.272165e-03     -1.587854e-01   \n",
       "75%         6.830606e-02         7.102765e-02      1.681711e-01   \n",
       "max         3.184967e+01         3.233498e+01      1.769901e+01   \n",
       "\n",
       "       acceleration_spread  gyro_pc_mean  gyro_pc_median   gyro_pc_std  \\\n",
       "count         3.852000e+03  3.852000e+03    3.852000e+03  3.852000e+03   \n",
       "mean         -1.195032e-17 -1.862621e-17    1.458393e-17 -1.572663e-16   \n",
       "std           1.000130e+00  1.000130e+00    1.000130e+00  1.000130e+00   \n",
       "min          -1.379592e+00 -1.043166e+01   -9.735176e+00 -7.270629e-01   \n",
       "25%          -4.625899e-01 -3.243065e-01   -1.786610e-01 -3.213616e-01   \n",
       "50%          -1.717348e-01 -1.138947e-02   -5.136919e-02 -1.958964e-01   \n",
       "75%           1.762076e-01  2.882491e-01    1.208569e-01 -2.988886e-02   \n",
       "max           1.528644e+01  1.382471e+01    1.179318e+01  1.604057e+01   \n",
       "\n",
       "       gyro_pc_spread    speed_mean  speed_median  ...   second_mean  \\\n",
       "count    3.852000e+03  3.852000e+03  3.852000e+03  ...  3.852000e+03   \n",
       "mean     6.953305e-18 -2.386893e-16 -1.102081e-16  ... -1.325812e-17   \n",
       "std      1.000130e+00  1.000130e+00  1.000130e+00  ...  1.000130e+00   \n",
       "min     -4.629994e-01 -1.865650e+00 -1.395112e+00  ... -1.602576e+00   \n",
       "25%     -2.587445e-01 -7.470991e-01 -7.792574e-01  ... -6.799369e-01   \n",
       "50%     -2.089942e-01 -2.424808e-01 -2.662417e-01  ... -1.474542e-01   \n",
       "75%     -1.315703e-01  5.971829e-01  6.360827e-01  ...  5.535852e-01   \n",
       "max      2.192246e+01  3.946116e+00  3.199149e+00  ...  2.370491e+01   \n",
       "\n",
       "       second_median    second_std  second_spread  num_non_speed_outlier  \\\n",
       "count   3.852000e+03  3.852000e+03   3.852000e+03           3.852000e+03   \n",
       "mean   -4.052372e-17 -2.556510e-17  -1.317165e-16           1.972001e-16   \n",
       "std     1.000130e+00  1.000130e+00   1.000130e+00           1.000130e+00   \n",
       "min    -1.568730e+00 -1.854968e+00  -1.842653e+00          -1.761975e+00   \n",
       "25%    -6.968992e-01 -7.427452e-01  -7.444038e-01          -7.706154e-01   \n",
       "50%    -1.492960e-01 -1.370580e-01  -1.459317e-01          -1.716688e-01   \n",
       "75%     5.266646e-01  6.472530e-01   6.609556e-01           6.234845e-01   \n",
       "max     2.264750e+01  6.607630e+00   6.626252e+00           7.748884e+00   \n",
       "\n",
       "       num_speed_outlier  num_non_accel_outlier  num_accel_outlier  \\\n",
       "count       3.852000e+03           3.852000e+03       3.852000e+03   \n",
       "mean        1.158644e-17           3.585455e-17      -4.340592e-17   \n",
       "std         1.000130e+00           1.000130e+00       1.000130e+00   \n",
       "min        -7.863839e-01          -1.724276e+00      -5.614917e-01   \n",
       "25%        -5.618027e-01          -7.563422e-01      -4.780220e-01   \n",
       "50%        -3.176928e-01          -1.637299e-01      -3.110827e-01   \n",
       "75%         1.509984e-01           6.190121e-01       5.061919e-02   \n",
       "max         1.085278e+01           7.678506e+00       1.051215e+01   \n",
       "\n",
       "       num_non_gyro_outlier  num_gyro_outlier  \n",
       "count          3.852000e+03      3.852000e+03  \n",
       "mean           1.339070e-16     -3.594823e-16  \n",
       "std            1.000130e+00      1.000130e+00  \n",
       "min           -1.710210e+00     -7.886986e-01  \n",
       "25%           -7.737081e-01     -4.612986e-01  \n",
       "50%           -1.723753e-01     -2.157487e-01  \n",
       "75%            6.359736e-01      1.116513e-01  \n",
       "max            7.704098e+00      1.652257e+01  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled = pd.DataFrame(scaler.fit_transform(X_test), columns = X_test.columns)\n",
    "X_test_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to build model\n",
    "\n",
    "Writing a function here so that the Keras sklearn wrapper can be used for the GridSearchCV (last section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_layers=1, num_nodes=32, lr=0.001, dropout=False, opt=\"SGD\", input_shape=(22,)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_nodes, activation='relu', input_shape=input_shape))\n",
    "    \n",
    "    if dropout:\n",
    "        model.add(Dropout(rate=0.5))\n",
    "    \n",
    "    if num_layers > 1:\n",
    "        for i in range(num_layers - 1):\n",
    "            model.add(Dense(num_nodes, activation='relu'))\n",
    "            if dropout:\n",
    "                model.add(Dropout(rate=0.5))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    if opt == \"SGD\":\n",
    "        optimiser = optimizers.SGD(lr=lr)\n",
    "    elif opt == \"RMSprop\":\n",
    "        optimiser = optimizers.RMSprop(lr=lr)\n",
    "    elif opt == \"Adam\":\n",
    "        optimiser = optimizers.Adam(lr=lr)\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", \n",
    "               optimizer=optimiser,\n",
    "               metrics=[\"accuracy\"])\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomised grid search for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "7939/7939 [==============================] - 2s 240us/step - loss: 0.6103 - accuracy: 0.6473\n",
      "Epoch 2/60\n",
      "7939/7939 [==============================] - 1s 157us/step - loss: 0.6039 - accuracy: 0.6539\n",
      "Epoch 3/60\n",
      "7939/7939 [==============================] - 1s 161us/step - loss: 0.5993 - accuracy: 0.6615\n",
      "Epoch 4/60\n",
      "7939/7939 [==============================] - 1s 163us/step - loss: 0.5964 - accuracy: 0.6603\n",
      "Epoch 5/60\n",
      "7939/7939 [==============================] - 1s 169us/step - loss: 0.5974 - accuracy: 0.6633\n",
      "Epoch 6/60\n",
      "7939/7939 [==============================] - 1s 169us/step - loss: 0.5970 - accuracy: 0.6593\n",
      "Epoch 7/60\n",
      "7939/7939 [==============================] - 1s 168us/step - loss: 0.5935 - accuracy: 0.6656\n",
      "Epoch 8/60\n",
      "7939/7939 [==============================] - 1s 172us/step - loss: 0.5919 - accuracy: 0.6627\n",
      "Epoch 9/60\n",
      "7939/7939 [==============================] - 1s 169us/step - loss: 0.5946 - accuracy: 0.6681\n",
      "Epoch 10/60\n",
      "7939/7939 [==============================] - 1s 181us/step - loss: 0.5929 - accuracy: 0.6702\n",
      "Epoch 11/60\n",
      "7939/7939 [==============================] - 1s 175us/step - loss: 0.5903 - accuracy: 0.6678\n",
      "Epoch 12/60\n",
      "7939/7939 [==============================] - 1s 177us/step - loss: 0.5894 - accuracy: 0.6668\n",
      "Epoch 13/60\n",
      "7939/7939 [==============================] - 1s 176us/step - loss: 0.5923 - accuracy: 0.6646\n",
      "Epoch 14/60\n",
      "7939/7939 [==============================] - 1s 170us/step - loss: 0.5893 - accuracy: 0.6638\n",
      "Epoch 15/60\n",
      "7939/7939 [==============================] - 1s 169us/step - loss: 0.5890 - accuracy: 0.6671\n",
      "Epoch 16/60\n",
      "7939/7939 [==============================] - 1s 169us/step - loss: 0.5936 - accuracy: 0.6652\n",
      "Epoch 17/60\n",
      "7939/7939 [==============================] - 1s 169us/step - loss: 0.5913 - accuracy: 0.6714\n",
      "Epoch 18/60\n",
      "7939/7939 [==============================] - 1s 169us/step - loss: 0.5913 - accuracy: 0.6678\n",
      "Epoch 19/60\n",
      "7939/7939 [==============================] - 1s 169us/step - loss: 0.5898 - accuracy: 0.6729\n",
      "Epoch 20/60\n",
      "7939/7939 [==============================] - 1s 170us/step - loss: 0.5892 - accuracy: 0.6741\n",
      "Epoch 21/60\n",
      "7939/7939 [==============================] - 1s 169us/step - loss: 0.5866 - accuracy: 0.6678\n",
      "Epoch 22/60\n",
      "7939/7939 [==============================] - 1s 169us/step - loss: 0.5905 - accuracy: 0.6735\n",
      "Epoch 23/60\n",
      "7939/7939 [==============================] - 1s 170us/step - loss: 0.5908 - accuracy: 0.6738\n",
      "Epoch 24/60\n",
      "7939/7939 [==============================] - 1s 169us/step - loss: 0.5887 - accuracy: 0.6705\n",
      "Epoch 25/60\n",
      "7939/7939 [==============================] - 1s 170us/step - loss: 0.5872 - accuracy: 0.6750\n",
      "Epoch 26/60\n",
      "7939/7939 [==============================] - 1s 177us/step - loss: 0.5893 - accuracy: 0.6734\n",
      "Epoch 27/60\n",
      "7939/7939 [==============================] - 1s 177us/step - loss: 0.5895 - accuracy: 0.6750\n",
      "Epoch 28/60\n",
      "7939/7939 [==============================] - 1s 177us/step - loss: 0.5855 - accuracy: 0.6739\n",
      "Epoch 29/60\n",
      "7939/7939 [==============================] - 1s 175us/step - loss: 0.5876 - accuracy: 0.6702\n",
      "Epoch 30/60\n",
      "7939/7939 [==============================] - 1s 169us/step - loss: 0.5853 - accuracy: 0.6699\n",
      "Epoch 31/60\n",
      "7939/7939 [==============================] - 1s 169us/step - loss: 0.5837 - accuracy: 0.6751\n",
      "Epoch 32/60\n",
      "7939/7939 [==============================] - 1s 170us/step - loss: 0.5910 - accuracy: 0.6787\n",
      "Epoch 33/60\n",
      "7939/7939 [==============================] - 1s 175us/step - loss: 0.5886 - accuracy: 0.6793\n",
      "Epoch 34/60\n",
      "7939/7939 [==============================] - 1s 169us/step - loss: 0.5890 - accuracy: 0.6717\n",
      "Epoch 35/60\n",
      "7939/7939 [==============================] - 1s 169us/step - loss: 0.5931 - accuracy: 0.6760\n",
      "Epoch 36/60\n",
      "7939/7939 [==============================] - 1s 170us/step - loss: 0.5892 - accuracy: 0.6715\n",
      "Epoch 37/60\n",
      "7939/7939 [==============================] - 1s 170us/step - loss: 0.5841 - accuracy: 0.6772\n",
      "Epoch 38/60\n",
      "7939/7939 [==============================] - 1s 169us/step - loss: 0.5847 - accuracy: 0.6760\n",
      "Epoch 39/60\n",
      "7939/7939 [==============================] - 1s 170us/step - loss: 0.5873 - accuracy: 0.6768\n",
      "Epoch 40/60\n",
      "7939/7939 [==============================] - 1s 174us/step - loss: 0.5841 - accuracy: 0.6767\n",
      "Epoch 41/60\n",
      "7939/7939 [==============================] - 1s 177us/step - loss: 0.5817 - accuracy: 0.6779\n",
      "Epoch 42/60\n",
      "7939/7939 [==============================] - 1s 177us/step - loss: 0.5797 - accuracy: 0.6777\n",
      "Epoch 43/60\n",
      "7939/7939 [==============================] - 1s 178us/step - loss: 0.5852 - accuracy: 0.6745\n",
      "Epoch 44/60\n",
      "7939/7939 [==============================] - 1s 172us/step - loss: 0.5950 - accuracy: 0.6764\n",
      "Epoch 45/60\n",
      "7939/7939 [==============================] - 1s 171us/step - loss: 0.5866 - accuracy: 0.6784\n",
      "Epoch 46/60\n",
      "7939/7939 [==============================] - 1s 169us/step - loss: 0.5932 - accuracy: 0.6794\n",
      "Epoch 47/60\n",
      "7939/7939 [==============================] - 1s 170us/step - loss: 0.5859 - accuracy: 0.6792\n",
      "Epoch 48/60\n",
      "7939/7939 [==============================] - 1s 169us/step - loss: 0.5849 - accuracy: 0.6763\n",
      "Epoch 49/60\n",
      "7939/7939 [==============================] - 1s 169us/step - loss: 0.5881 - accuracy: 0.6768\n",
      "Epoch 50/60\n",
      "7939/7939 [==============================] - 1s 169us/step - loss: 0.5796 - accuracy: 0.6838\n",
      "Epoch 51/60\n",
      "7939/7939 [==============================] - 1s 170us/step - loss: 0.5856 - accuracy: 0.6772\n",
      "Epoch 52/60\n",
      "7939/7939 [==============================] - 1s 170us/step - loss: 0.5807 - accuracy: 0.6782\n",
      "Epoch 53/60\n",
      "7939/7939 [==============================] - 1s 170us/step - loss: 0.5894 - accuracy: 0.6773\n",
      "Epoch 54/60\n",
      "7939/7939 [==============================] - 1s 177us/step - loss: 0.5921 - accuracy: 0.6765\n",
      "Epoch 55/60\n",
      "7939/7939 [==============================] - 1s 178us/step - loss: 0.5832 - accuracy: 0.6775\n",
      "Epoch 56/60\n",
      "7939/7939 [==============================] - 1s 183us/step - loss: 0.5837 - accuracy: 0.6825\n",
      "Epoch 57/60\n",
      "7939/7939 [==============================] - 1s 181us/step - loss: 0.5869 - accuracy: 0.6808\n",
      "Epoch 58/60\n",
      "7939/7939 [==============================] - 1s 175us/step - loss: 0.5828 - accuracy: 0.6818\n",
      "Epoch 59/60\n",
      "7939/7939 [==============================] - 1s 170us/step - loss: 0.5895 - accuracy: 0.6755\n",
      "Epoch 60/60\n",
      "7939/7939 [==============================] - 1s 174us/step - loss: 0.5797 - accuracy: 0.6812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x1a4ce18650>,\n",
       "                   iid='warn', n_iter=10, n_jobs=-1,\n",
       "                   param_distributions={'dropout': [True, False],\n",
       "                                        'lr': [0.0001, 0.001, 0.01, 0.1],\n",
       "                                        'num_layers': [1, 2],\n",
       "                                        'num_nodes': [64, 128, 256, 512],\n",
       "                                        'opt': ['SGD', 'RMSprop', 'Adam']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=199, refit=True,\n",
       "                   return_train_score=False, scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# keras.wrappers.scikit_learn.KerasClassifier(build_fn=None, **sk_params)\n",
    "\n",
    "params = {'num_nodes' : [64, 128, 256, 512],\n",
    "         'num_layers' : [1, 2], \n",
    "         'lr' : [0.0001, 0.001, 0.01, 0.1],\n",
    "         'dropout' : [True, False],\n",
    "         'opt' : [\"SGD\", \"RMSprop\", \"Adam\"]}\n",
    "\n",
    "model = KerasClassifier(build_fn=build_model, epochs=60, batch_size=32)\n",
    "# grid = GridSearchCV(model, param_grid=params, n_jobs=4, cv=5)\n",
    "# grid.fit(X_train, y_train)\n",
    "\n",
    "seed = 199\n",
    "num_iter = 10\n",
    "\n",
    "random_grid = RandomizedSearchCV(estimator=model, \n",
    "                                 param_distributions=params, \n",
    "                                 scoring='roc_auc', n_iter=num_iter, cv=5, n_jobs=-1,\n",
    "                                 random_state=seed)\n",
    "random_grid.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7341532702727198\n",
      "{'opt': 'RMSprop', 'num_nodes': 512, 'num_layers': 1, 'lr': 0.001, 'dropout': True}\n"
     ]
    }
   ],
   "source": [
    "# This is actually worse than the current params, so ignore\n",
    "print(random_grid.best_score_)\n",
    "print(random_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross validation of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1 of 5\n",
      "Total time taken: 67 seconds\n",
      "Processing fold 2 of 5\n",
      "Total time taken: 137 seconds\n",
      "Processing fold 3 of 5\n",
      "Total time taken: 213 seconds\n",
      "Processing fold 4 of 5\n",
      "Total time taken: 294 seconds\n",
      "Processing fold 5 of 5\n",
      "Total time taken: 373 seconds\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "k = 5\n",
    "num_val_samples = X_train_scaled.shape[0] // k\n",
    "num_epochs = 60\n",
    "batch_size = 32\n",
    "all_val_loss = []\n",
    "all_train_loss = []\n",
    "all_val_acc = []\n",
    "all_train_acc = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('Processing fold {} of {}'.format(i + 1, k))\n",
    "\n",
    "    val_data = X_train_scaled[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    partial_train_data = np.concatenate([X_train_scaled[:i * num_val_samples], \n",
    "                                         X_train_scaled[(i + 1) * num_val_samples:]], axis=0)\n",
    "    partial_train_targets = np.concatenate([y_train[:i * num_val_samples], \n",
    "                                            y_train[(i + 1) * num_val_samples:]], axis=0)\n",
    "    \n",
    "\n",
    "    model = build_model(num_layers=2, num_nodes=128, lr=0.0001, opt=\"Adam\", dropout=True, input_shape=((22,)))    \n",
    "    history = model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, verbose=0,\n",
    "                        validation_data=((val_data, val_targets)), batch_size=batch_size)\n",
    "\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    acc = history.history[\"accuracy\"]\n",
    "    val_acc = history.history[\"val_accuracy\"]\n",
    "\n",
    "    all_val_loss.append(val_loss)\n",
    "    all_train_loss.append(loss)\n",
    "    all_val_acc.append(val_acc)\n",
    "    all_train_acc.append(acc)\n",
    "\n",
    "    end_time = datetime.datetime.now()\n",
    "    print(\"Total time taken: {} seconds\".format((end_time - start_time).seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5935621542053799, train acc: 0.6637279391288757\n",
      "Val loss: 0.5974864174978645, val acc: 0.6594833016395569\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "average_val_loss = [np.mean([x[i] for x in all_val_loss]) for i in range(num_epochs)]\n",
    "average_train_loss = [np.mean([x[i] for x in all_train_loss]) for i in range(num_epochs)]\n",
    "\n",
    "average_val_acc = [np.mean([x[i] for x in all_val_acc]) for i in range(num_epochs)]\n",
    "average_train_acc = [np.mean([x[i] for x in all_train_acc]) for i in range(num_epochs)]\n",
    "\n",
    "print(\"Train loss: {}, train acc: {}\".format(average_train_loss[-1], average_train_acc[-1]))\n",
    "print(\"Val loss: {}, val acc: {}\".format(average_val_loss[-1], average_val_acc[-1]))\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.plot(range(1, len(average_train_loss) + 1), average_train_loss, color=\"r\", label=\"Training loss\")\n",
    "plt.plot(range(1, len(average_val_loss) + 1), average_val_loss, color=\"b\", label=\"Validation loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing predictions after getting a satisfactory model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounded test ROC score : 0.6853582554517134\n",
      "Un-rounded test ROC score : 0.7499837353487842\n"
     ]
    }
   ],
   "source": [
    "# model = build_model(num_layers=2, num_nodes=128, lr=0.0001, opt=\"Adam\", dropout=True, input_shape=((22,)))\n",
    "# history = model.fit(X_train_scaled, y_train, epochs=45, batch_size=batch_size, verbose=0)\n",
    "\n",
    "res = model.predict(X_test_scaled)\n",
    "res2 = [round(num[0]) for num in res]\n",
    "\n",
    "print(\"Rounded test ROC score : {}\".format(roc_auc_score(y_test, res2)))\n",
    "print(\"Un-rounded test ROC score : {}\".format(roc_auc_score(y_test, res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('models/ann_weights.h5')\n",
    "\n",
    "with open('models/ann_architecture.json', 'w') as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1758, 1131],\n",
       "       [ 229,  734]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Horizontal = y_pred\n",
    "# Vertical = y_true\n",
    "\n",
    "confusion_matrix(y_test, res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
